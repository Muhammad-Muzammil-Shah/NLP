{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3afe14db-1ca7-4027-850f-f212e081ca15",
   "metadata": {},
   "source": [
    "#              Quiz No-01 NLP\n",
    "## M Muzammil Shah BAI-22S-028\n",
    "## Yureedullah     BAI-22S-044"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0371a68b-75db-4a01-95a4-2e538909fddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import nltk.corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "170a7ecd-a897-4906-bacb-610cebf496df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c267972-c4b0-49ab-80cc-b189a2d3c572",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThis is my class\\nClass is good\\nClass is NLP\\nNLP is good subject\\nSubject has text knowledge\\nKnowledge is power\\nText has many algorithms\\nAlgorithms like bog N-gram etc\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_string=\"\"\"\n",
    "This is my class\n",
    "Class is good\n",
    "Class is NLP\n",
    "NLP is good subject\n",
    "Subject has text knowledge\n",
    "Knowledge is power\n",
    "Text has many algorithms\n",
    "Algorithms like bog N-gram etc\n",
    "\"\"\"\n",
    "my_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64f93c5d-d8b8-4994-b401-9323e8bdd6c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'is',\n",
       " 'my',\n",
       " 'class',\n",
       " 'Class',\n",
       " 'is',\n",
       " 'good',\n",
       " 'Class',\n",
       " 'is',\n",
       " 'NLP',\n",
       " 'NLP',\n",
       " 'is',\n",
       " 'good',\n",
       " 'subject',\n",
       " 'Subject',\n",
       " 'has',\n",
       " 'text',\n",
       " 'knowledge',\n",
       " 'Knowledge',\n",
       " 'is',\n",
       " 'power',\n",
       " 'Text',\n",
       " 'has',\n",
       " 'many',\n",
       " 'algorithms',\n",
       " 'Algorithms',\n",
       " 'like',\n",
       " 'bog',\n",
       " 'N-gram',\n",
       " 'etc']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strint_tokens=nltk.word_tokenize(my_string)\n",
    "strint_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8bd0169-c5bd-40bd-8651-6f303263dd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#freguency of tokens\n",
    "from nltk.probability import FreqDist\n",
    "fdist=FreqDist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a39217ae-be22-4be1-914e-4d99f8727adc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('is', 5),\n",
       " ('Class', 2),\n",
       " ('good', 2),\n",
       " ('NLP', 2),\n",
       " ('has', 2),\n",
       " ('This', 1),\n",
       " ('my', 1),\n",
       " ('class', 1),\n",
       " ('subject', 1),\n",
       " ('Subject', 1),\n",
       " ('text', 1),\n",
       " ('knowledge', 1),\n",
       " ('Knowledge', 1),\n",
       " ('power', 1),\n",
       " ('Text', 1),\n",
       " ('many', 1),\n",
       " ('algorithms', 1),\n",
       " ('Algorithms', 1),\n",
       " ('like', 1),\n",
       " ('bog', 1),\n",
       " ('N-gram', 1),\n",
       " ('etc', 1)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in strint_tokens:\n",
    "    fdist[i]=fdist[i]+1\n",
    "fdist.most_common()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b6d0ff7-7014-480c-9a91-3b3058f6ae53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('This', 'is'),\n",
       " ('is', 'my'),\n",
       " ('my', 'class'),\n",
       " ('class', 'Class'),\n",
       " ('Class', 'is'),\n",
       " ('is', 'good'),\n",
       " ('good', 'Class'),\n",
       " ('Class', 'is'),\n",
       " ('is', 'NLP'),\n",
       " ('NLP', 'NLP'),\n",
       " ('NLP', 'is'),\n",
       " ('is', 'good'),\n",
       " ('good', 'subject'),\n",
       " ('subject', 'Subject'),\n",
       " ('Subject', 'has'),\n",
       " ('has', 'text'),\n",
       " ('text', 'knowledge'),\n",
       " ('knowledge', 'Knowledge'),\n",
       " ('Knowledge', 'is'),\n",
       " ('is', 'power'),\n",
       " ('power', 'Text'),\n",
       " ('Text', 'has'),\n",
       " ('has', 'many'),\n",
       " ('many', 'algorithms'),\n",
       " ('algorithms', 'Algorithms'),\n",
       " ('Algorithms', 'like'),\n",
       " ('like', 'bog'),\n",
       " ('bog', 'N-gram'),\n",
       " ('N-gram', 'etc')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(nltk.bigrams(strint_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "edf47207-939d-4d22-ac80-5233628143c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram Model:\n",
      "this: 0.0333\n",
      "is: 0.1667\n",
      "my: 0.0333\n",
      "class: 0.1000\n",
      "good: 0.0667\n",
      "nlp: 0.0667\n",
      "subject: 0.0667\n",
      "has: 0.0667\n",
      "text: 0.0667\n",
      "knowledge: 0.0667\n",
      "power: 0.0333\n",
      "many: 0.0333\n",
      "algorithms: 0.0667\n",
      "like: 0.0333\n",
      "bog: 0.0333\n",
      "n-gram: 0.0333\n",
      "etc: 0.0333\n",
      "\n",
      "Probability of the sentence 'NLP is fun': 0.0\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Text to analyze\n",
    "text = my_string\n",
    "\n",
    "# Step 1: Tokenize the text\n",
    "text = text.lower()  # Convert to lowercase\n",
    "tokens = text.split()  # Split the text into words\n",
    "\n",
    "# Step 2: Build the unigram model\n",
    "total_words = len(tokens)  # Total number of words\n",
    "word_counts = Counter(tokens)  # Count the frequency of each word\n",
    "\n",
    "# Calculate the probability of each word\n",
    "unigram_model = {}\n",
    "for word, count in word_counts.items():\n",
    "    unigram_model[word] = count / total_words\n",
    "\n",
    "# Display the unigram model\n",
    "print(\"Unigram Model:\")\n",
    "for word, prob in unigram_model.items():\n",
    "    print(f\"{word}: {prob:.4f}\")\n",
    "\n",
    "# Step 3: Calculate the probability of a sentence\n",
    "sentence = \"NLP is fun\"\n",
    "sentence_tokens = sentence.lower().split()  # Tokenize the sentence\n",
    "\n",
    "# Start with probability 1 and multiply by each word's probability\n",
    "probability = 1\n",
    "for word in sentence_tokens:\n",
    "    probability *= unigram_model.get(word, 0)  # Use 0 if word not in model\n",
    "\n",
    "print(f\"\\nProbability of the sentence '{sentence}': {probability}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "04fb468c-b8a7-4346-90b1-1b21a4ab9c16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram Model:\n",
      "this is: 1.0000\n",
      "is my: 0.2000\n",
      "my class: 1.0000\n",
      "class class: 0.3333\n",
      "class is: 0.6667\n",
      "is good: 0.4000\n",
      "good class: 0.5000\n",
      "is nlp: 0.2000\n",
      "nlp nlp: 0.5000\n",
      "nlp is: 0.5000\n",
      "good subject: 0.5000\n",
      "subject subject: 0.5000\n",
      "subject has: 0.5000\n",
      "has text: 0.5000\n",
      "text knowledge: 0.5000\n",
      "knowledge knowledge: 0.5000\n",
      "knowledge is: 0.5000\n",
      "is power: 0.2000\n",
      "power text: 1.0000\n",
      "text has: 0.5000\n",
      "has many: 0.5000\n",
      "many algorithms: 1.0000\n",
      "algorithms algorithms: 0.5000\n",
      "algorithms like: 0.5000\n",
      "like bog: 1.0000\n",
      "bog n-gram: 1.0000\n",
      "n-gram etc: 1.0000\n",
      "\n",
      "Probability of the sentence 'NLP is fun': 0.0\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Text to analyze\n",
    "text = my_string\n",
    "\n",
    "# Step 1: Tokenize the text\n",
    "text = text.lower()  # Convert to lowercase\n",
    "tokens = text.split()  # Split the text into words\n",
    "\n",
    "# Step 2: Build the bigram model\n",
    "bigrams = [(tokens[i], tokens[i+1]) for i in range(len(tokens) - 1)]  # Create bigrams (pairs of consecutive words)\n",
    "bigram_counts = Counter(bigrams)  # Count each bigram's frequency\n",
    "\n",
    "# Count the frequency of individual words (unigrams) for normalizing the bigram probabilities\n",
    "word_counts = Counter(tokens)\n",
    "\n",
    "# Calculate bigram probabilities\n",
    "bigram_model = {}\n",
    "for (w1, w2), count in bigram_counts.items():\n",
    "    bigram_model[(w1, w2)] = count / word_counts[w1]  # P(w2 | w1) = count(w1, w2) / count(w1)\n",
    "\n",
    "# Display the bigram model\n",
    "print(\"Bigram Model:\")\n",
    "for (w1, w2), prob in bigram_model.items():\n",
    "    print(f\"{w1} {w2}: {prob:.4f}\")\n",
    "\n",
    "# Step 3: Calculate the probability of a sentence\n",
    "sentence = \"NLP is fun\"\n",
    "sentence_tokens = sentence.lower().split()  # Tokenize the sentence\n",
    "\n",
    "# Start with probability 1 and multiply by each bigram's probability\n",
    "probability = 1\n",
    "for i in range(len(sentence_tokens) - 1):\n",
    "    w1, w2 = sentence_tokens[i], sentence_tokens[i + 1]\n",
    "    probability *= bigram_model.get((w1, w2), 0)  # Use 0 if bigram not in model\n",
    "\n",
    "print(f\"\\nProbability of the sentence '{sentence}': {probability}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4b8f62ed-7cdc-48f8-b1f0-2b86a66376ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram Probabilities:\n",
      "P(is | this) = 1.0000\n",
      "P(my | is) = 0.2000\n",
      "P(class | my) = 1.0000\n",
      "P(is | class) = 0.6667\n",
      "P(good | is) = 0.4000\n",
      "P(nlp | is) = 0.2000\n",
      "P(is | nlp) = 0.5000\n",
      "P(subject | good) = 0.5000\n",
      "P(has | subject) = 0.5000\n",
      "P(text | has) = 0.5000\n",
      "P(knowledge | text) = 0.5000\n",
      "P(is | knowledge) = 0.5000\n",
      "P(power | is) = 0.2000\n",
      "P(has | text) = 0.5000\n",
      "P(many | has) = 0.5000\n",
      "P(algorithms | many) = 1.0000\n",
      "P(like | algorithms) = 0.5000\n",
      "P(bog | like) = 1.0000\n",
      "P(n-gram | bog) = 1.0000\n",
      "P(etc | n-gram) = 1.0000\n",
      "\n",
      "Bigram Counts:\n",
      "('this', 'is'): 1\n",
      "('is', 'my'): 1\n",
      "('my', 'class'): 1\n",
      "('class', 'is'): 2\n",
      "('is', 'good'): 2\n",
      "('is', 'nlp'): 1\n",
      "('nlp', 'is'): 1\n",
      "('good', 'subject'): 1\n",
      "('subject', 'has'): 1\n",
      "('has', 'text'): 1\n",
      "('text', 'knowledge'): 1\n",
      "('knowledge', 'is'): 1\n",
      "('is', 'power'): 1\n",
      "('text', 'has'): 1\n",
      "('has', 'many'): 1\n",
      "('many', 'algorithms'): 1\n",
      "('algorithms', 'like'): 1\n",
      "('like', 'bog'): 1\n",
      "('bog', 'n-gram'): 1\n",
      "('n-gram', 'etc'): 1\n",
      "\n",
      "Unigram Counts:\n",
      "this: 1\n",
      "is: 5\n",
      "my: 1\n",
      "class: 3\n",
      "good: 2\n",
      "nlp: 2\n",
      "subject: 2\n",
      "has: 2\n",
      "text: 2\n",
      "knowledge: 2\n",
      "power: 1\n",
      "many: 1\n",
      "algorithms: 2\n",
      "like: 1\n",
      "bog: 1\n",
      "n-gram: 1\n",
      "etc: 1\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Step 1: Input text and split by lines\n",
    "text = \"\"\"\n",
    "This is my class\n",
    "Class is good\n",
    "Class is NLP\n",
    "NLP is good subject\n",
    "Subject has text knowledge\n",
    "Knowledge is power\n",
    "Text has many algorithms\n",
    "Algorithms like bog N-gram etc\n",
    "\"\"\"\n",
    "\n",
    "# Step 2: Tokenize and process bigrams line by line\n",
    "lines = text.strip().lower().split('\\n')  # Split text into lines\n",
    "\n",
    "# Initialize lists for storing all tokens and bigrams\n",
    "all_tokens = []\n",
    "all_bigrams = []\n",
    "\n",
    "# Process each line and extract bigrams\n",
    "for line in lines:\n",
    "    tokens = line.split()  # Tokenize the current line\n",
    "    all_tokens.extend(tokens)  # Collect tokens for word count\n",
    "    bigrams = [(tokens[i], tokens[i + 1]) for i in range(len(tokens) - 1)]  # Create bigrams for the line\n",
    "    all_bigrams.extend(bigrams)  # Collect all bigrams\n",
    "\n",
    "# Step 3: Count bigram and word frequencies\n",
    "bigram_counts = Counter(all_bigrams)  # Count bigram frequencies\n",
    "word_counts = Counter(all_tokens)  # Count word (unigram) frequencies\n",
    "\n",
    "# Step 4: Calculate bigram probabilities\n",
    "bigram_probabilities = {}\n",
    "for (w1, w2), count in bigram_counts.items():\n",
    "    bigram_probabilities[(w1, w2)] = count / word_counts[w1]  # P(w2 | w1) = count(w1, w2) / count(w1)\n",
    "\n",
    "# Step 5: Display bigram probabilities\n",
    "print(\"Bigram Probabilities:\")\n",
    "for (w1, w2), prob in bigram_probabilities.items():\n",
    "    print(f\"P({w2} | {w1}) = {prob:.4f}\")\n",
    "\n",
    "# Optional: Display bigram and word counts for reference\n",
    "print(\"\\nBigram Counts:\")\n",
    "for bigram, count in bigram_counts.items():\n",
    "    print(f\"{bigram}: {count}\")\n",
    "\n",
    "print(\"\\nUnigram Counts:\")\n",
    "for word, count in word_counts.items():\n",
    "    print(f\"{word}: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e56a9d8-0c49-4d92-acc1-ef39b6362f79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
